import streamlit as st

# â˜… í˜ì´ì§€ ì„¤ì •ì€ ìµœìƒë‹¨ì—ì„œ ë‹¨ í•œ ë²ˆ!
st.set_page_config(
    page_title="ì£¼ì‹ ë‰´ìŠ¤ ìš”ì•½ ë³´ë“œ",
    layout="wide",
    page_icon="ğŸ“°",
)

import nltk
# ì´ë¯¸ ìˆìœ¼ë©´ ë°”ë¡œ í†µê³¼, ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

import pandas as pd
from news_crawler import crawl_google_news, get_article_text
from summarizer import summarize
from price_fetcher import fetch_prices
from streamlit_autorefresh import st_autorefresh

# â¬‡ï¸ 5ë¶„ë§ˆë‹¤ ì˜¤í† ë¦¬í”„ë ˆì‹œ
st_autorefresh(interval=300_000, key="auto_refresh")

# â¬‡ï¸ ìºì‹±ëœ í•¨ìˆ˜ë“¤ ì„ ì–¸(load_summarizer, cached_crawl, ...)

# 1) ë¬´ê±°ìš´ ëª¨ë¸ì€ 'ìì›' ìºì‹œ
@st.cache_resource
def load_summarizer():
    from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast
    model_name = "digit82/kobart-summarization"
    tok  = PreTrainedTokenizerFast.from_pretrained(model_name)
    bart = BartForConditionalGeneration.from_pretrained(model_name)
    return tok, bart

tokenizer, model = load_summarizer()

def summarize_cached(text:str, max_len=1024, summary_len=128) -> str:
    """ìš”ì•½ ì „ìš© ë˜í¼ â€’ ëª¨ë¸Â·í† í¬ë‚˜ì´ì €ëŠ” ì´ë¯¸ ìºì‹±ë¨"""
    # ë°ì´í„° ê²°ê³¼ë§Œ ìºì‹±
    @st.cache_data(ttl=24*60*60)   # 24ì‹œê°„ ìœ ì§€
    def _summ(text):
        ids = tokenizer.encode(text, max_length=max_len,
                               truncation=True, return_tensors="pt")
        out = model.generate(ids, max_length=summary_len,
                             num_beams=4, early_stopping=True)
        return tokenizer.decode(out[0], skip_special_tokens=True)
    return _summ(text)

# 2) ë‰´ìŠ¤ í¬ë¡¤ë§Â·ë³¸ë¬¸ ì¶”ì¶œÂ·ì£¼ê°€ë„ ì „ë¶€ 'ë°ì´í„°' ìºì‹œ
@st.cache_data(ttl=3600)
def cached_crawl(keyword):
    return crawl_google_news(keyword, max_items=3)

@st.cache_data(ttl=24*60*60)       # 24ì‹œê°„ ìœ ì§€
def cached_article(url:str):
    return get_article_text(url)   # newspaper3k ì‚¬ìš©

@st.cache_data(ttl=30*60)          # 30ë¶„ ìœ ì§€
def cached_price(ticker:str):
    return fetch_prices(ticker, period="3mo")

@st.cache_data(ttl=24*60*60)       # 24ì‹œê°„ ìœ ì§€
def cached_summary(text:str) -> str:
    return summarize(text)

#################################################################
# 1) M7 ë§¤í•‘ í…Œì´ë¸” ì¶”ê°€
#################################################################
MAG7 = {
    "AAPL":  "ì• í”Œ",
    "MSFT":  "ë§ˆì´í¬ë¡œì†Œí”„íŠ¸",
    "AMZN":  "ì•„ë§ˆì¡´",
    "GOOGL": "êµ¬ê¸€",          # ì•ŒíŒŒë²³(í´ë˜ìŠ¤A)
    "META":  "ë©”íƒ€",
    "TSLA":  "í…ŒìŠ¬ë¼",
    "NVDA":  "ì—”ë¹„ë””ì•„",
}

#################################################################
# 2) ê´€ì‹¬ ì¢…ëª© ì„ íƒ UI í™•ì¥
#################################################################
st.sidebar.header("ê´€ì‹¬ ì¢…ëª© ì„¤ì •")

# "M7 ì „ì²´" í•œ ë²ˆì— ë¶ˆëŸ¬ì˜¤ê¸°ìš© ì²´í¬ë°•ìŠ¤
use_mag7 = st.sidebar.checkbox("ğŸ’« Magnificent 7 ì „ì²´ ë³´ê¸°", value=True)

# ê¸°ë³¸ í›„ë³´ ëª©ë¡
tickers_default = list(MAG7.keys())
# ë©€í‹°ì…€ë ‰íŠ¸ â€“ M7 ì„ íƒ ì—¬ë¶€ ë”°ë¼ ê¸°ë³¸ê°’ ì¡°ì •
tickers = st.sidebar.multiselect(
    "í‹°ì»¤ë¥¼ ì„ íƒ/ì¶”ê°€í•˜ì„¸ìš”",
    options=tickers_default + ["005930.KS", "035420.KS"],   # í•œêµ­ ì£¼ì‹ ë“± ì¶”ê°€ ê°€ëŠ¥
    default=tickers_default if use_mag7 else [],
)

# M7 ì „ìš© ì„¤ëª…
if use_mag7:
    st.sidebar.caption("M7: AAPLÂ·MSFTÂ·AMZNÂ·GOOGLÂ·METAÂ·TSLAÂ·NVDA")

#################################################################
# ì‚¬ì´ë“œë°” í•˜ë‹¨ : ë‰´ìŠ¤ ìš”ì•½ ì˜ì—­
#################################################################
st.sidebar.divider()
st.sidebar.subheader("ğŸ“° ìš”ì•½ ë‰´ìŠ¤ (ìµœê·¼ 1ê±´)")

max_sidebar_news = 1          # ì‚¬ì´ë“œë°”ì—ëŠ” ì¢…ëª©ë‹¹ 1ê°œë§Œ
for t in tickers:
    keyword = MAG7.get(t, t.split('.')[0])
    news_df = cached_crawl(keyword).head(max_sidebar_news)
    if news_df.empty:
        continue

    # ê¸°ì‚¬ 1ê±´ì”© ìš”ì•½
    row = news_df.iloc[0]
    with st.sidebar.expander(f"{keyword} Â· {row['title'][:25]}..."):
        st.sidebar.write(row["url"])

        # ìš”ì•½ì€ ìºì‹±ë˜ì–´ ìˆìœ¼ë©´ ì¦‰ì‹œ, ì—†ìœ¼ë©´ 1-2ì´ˆ
        try:
            full = cached_article(row["url"])
            if cached_summary(full):    # ì´ë¯¸ ìºì‹œëœ ê²½ìš°
                st.sidebar.write(cached_summary(full))
            else:
                with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
                    summ = cached_summary(full)
                    st.sidebar.write(summ)
        except Exception as e:
            summ = f"ìš”ì•½ ì˜¤ë¥˜: {e}"
            st.sidebar.write(summ)

        # â‘¡ ë””ë²„ê·¸: ìš”ì•½ ê²°ê³¼ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶œë ¥
        st.text("ìš”ì•½ DEBUG â†’ " + repr(summ)[:150])

#################################################################
# 1) ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ Â· ìš”ì•½  (ë©”ì¸ í™”ë©´ ì˜¤ë¥¸ìª½)
#################################################################
st.header("ğŸ“° ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ Â· ìš”ì•½")

for t in tickers:
    keyword = MAG7.get(t, t.split('.')[0])
    news_df = cached_crawl(keyword).head(3)

    # â”€ ë””ë²„ê·¸ â”€
    st.write("DEBUG:", keyword, len(news_df))

    if news_df.empty:
        continue

    st.subheader(f"ğŸ”– {keyword}")

    for _, row in news_df.iterrows():
        st.markdown(f"**{row['title']}**  \n[{row['url']}]({row['url']})")

        full_text = cached_article(row["url"])
        st.caption(f"ë³¸ë¬¸ ê¸¸ì´: {len(full_text)}")      # ë””ë²„ê·¸

        if not full_text:
            st.warning("ğŸ”— ë³¸ë¬¸ íŒŒì‹± ì‹¤íŒ¨ â€“ ì›ë¬¸ ë§í¬ë¡œ ì´ë™í•´ ì£¼ì„¸ìš”.")
        else:
            st.write(cached_summary(full_text))

        st.markdown("---")

# 3) ì£¼ê°€ ì‹œì„¸
st.header("ğŸ“ˆ ì£¼ê°€ ì°¨íŠ¸")
for t in tickers:
    st.subheader(f"ğŸ“Š {t} ì¢…ê°€ ì¶”ì´")
    price_df = cached_price(t)
    st.line_chart(price_df.set_index("Date")["Close"], height=180)
    st.caption(f"ê¸°ê°„: {price_df['Date'].min().date()} ~ {price_df['Date'].max().date()}") 